1) implementare deliberation cost 
2) no rete per scelta policy ma calcolo Q(s, o) per ciascuna opzione e poi pigreco = argmax(Q(s, o)), po calcoli advantage se negativo aumenta prob di terminazione.


3) le opzioni sono il modo di comportarsi dell'agente (se si imparano) quindi non sono template di portafoglio ma modi di agire. PROBLEMA: in questo modo si è più vicini al trading e ci si allontana dal portfolio management.
SOLUZIONE: determini tu le opzioni (non learnable) che sono dei template di portafoglio (pesi non delle singole stock ma struttura del portafoglio, tipo 60 azionario 40 obbligazionario...). Bisogna informarsi su come gli investitori passivi modificano il portafoglio (hanno come riferimento delle strutture di portafoglio? se si quelle sono le nostre opzioni).
il low level qui sarebbe modificare i pesi in modo che il portafoglio corrente assomigli il più possibile a quello indicato dall'opzione.
IL REWARD: sarebbe una combinazione tra il profitto ottenuto e la vicinanza del portafoglio corrente a quello indicato dall'opzione. 
Se la distanza tra il portafoglio corrente e quello indicato è sotto una soglia allora il low level si ferma (è obbligato a non compiere nessuna azione, fai hard code per imporre questo).
In questo modo basta un deliberation cost nell'high level dato il low level deve solo preoccuparsi di far somigliare il portafoglio corrente a quello indicato senza badare ai costi (come sarebbe nella realtà). Al contrario se le opzioni fossero solo tendenze di comportamento dell'agente, e quindi sarebbe trading (perché non c'è un portafoglio obiettivo ma le opzioni probabilmente sarebbero solo compra più azioni o compra più obbligazioni), allora si dovrebbero considerare i costi di transazione anche nel low level.

4) Usare LLM: Usare un LLM che ti classifica gli articoli finanziari (l'articolo descrive fase espansiva dell'economia, crisi, guerre, fase di recessione) questa classificazione va ad influenzare la scelta di cambiare (e nel caso anche la scelta di quale usare al suo posto) l'opzione.




Qui sotto codici cancellati


Option_policies.py

import torch 
import torch.nn as nn
import torch.nn.functional as F
# INCLUDI QUESTO FILE IN NETWORKS

class OptionPolicy(nn.Module):
    """This is the high level policy over options pi(o|s)
    The output will be logits over K options"""
    def __init__(self, obs_dim, num_options, hidden = 256):
        super().__init__()
        self.affine1 = nn.Linear(obs_dim, hidden)
        self.affine2 = nn.Linear(hidden, hidden)
        self.logits = nn.Linear(hidden, num_options)

    def forward(self, s):
        x = F.relu(self.affine1(s))
        x = F.relu(self.affine2(x))
        return self.logits(x)
    """the input is a tensor of states (B, obs_dim) and the output is (B, K) so for each batch you obtain a vector of K numbers,
    the higher the number the more that option is preferred"""

class Termination(nn.Module):
    """This is the termination function beta(s, o): probability of terminating option o in state s
    The output will be K logits so each logit is how likely the respective option will end"""
    def __init__(self, obs_dim, num_options, hidden = 256):
        super().__init__()
        self.affine1 = nn.Linear(obs_dim, hidden)
        self.affine2 = nn.Linear(hidden, hidden)
        self.logits = nn.Linear(hidden, num_options)

    def forward(self, s):
        x = F.relu(self.affine1(s))
        x = F.relu(self.affine2(x))
        return self.logits(x) # the output is (B, K)
    
    def beta(self, s, option):
        logits = self.forward(s) # (B, K) terminations for all the options
        # For each batch row i, we want the logit at column option[i]
        B = logits.size(0)
        rows = torch.arange(B, device=logits.device)   
        opt_logits = logits[rows, option]              
        opt_logits = opt_logits.unsqueeze(1)           

        return torch.sigmoid(opt_logits) # in this way from logits we get probability (if logits is big then Beta close to 1 and viceversa)


